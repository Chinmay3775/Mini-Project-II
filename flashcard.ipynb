{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIkxOOROtzfB",
        "outputId": "09f82cca-b3b8-4562-e307-a9ed838cf0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“š Generated Flashcards:\n",
            "\n",
            "It is the only instance in commercial nuclear power history where radiation-related fatalities occurred.[9][10] As of 2005, 6000 cases of childhood thyroid cancer occurred within the affected populations, \"a large fraction\" being attributed to the disaster.[11] The United Nations Scientific Committee on the Effects of Atomic Radiation estimates fewer than 100 deaths have resulted from the fallout.[12] Predictions of the eventual total death toll vary; a 2006 World Health Organization study projected 9,000 cancer-related fatalities in Ukraine, Belarus, and Russia.[13]  Pripyat was abandoned and replaced by the purpose-built city of Slavutych.\n",
            "\n",
            "In case of a total power loss, each of Chernobyl's reactors had three backup diesel generators, but they took 60â€“75 seconds to reach full load and generate the 5.5 MW needed to run one main pump.[18]:â€Š15â€Š Special counterweights on each pump provided coolant via inertia to bridge the gap to generator startup.[19][20]\n",
            "\n",
            "RBMK reactors, like those at Chernobyl, use water as a coolant, circulated by electrically driven pumps.[16][17] Reactor No. 4 had 1,661 individual fuel channels, requiring over 12 million US gallons (45 million litres) per hour for the entire reactor.\n",
            "\n",
            "This would not quite bridge the gap between an external power failure and the full availability of the emergency generators, but would alleviate the situation.[22]  Safety test The turbine run-down energy capability still needed to be confirmed experimentally, and previous tests had ended unsuccessfully.\n",
            "\n",
            "Due to decay heat, solid fuel power reactors need high flows of coolant after a fission shutdown for a considerable time to prevent fuel cladding damage, or in the worst case, a full core meltdown.\n",
            "\n",
            "An initial test carried out in 1982 indicated that the excitation voltage of the turbine-generator was insufficient.\n",
            "\n",
            "16\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Splits text into sentences while maintaining structure and meaning.\"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return sentences\n",
        "\n",
        "def extract_key_phrases(text):\n",
        "    \"\"\"Extracts key phrases using Named Entity Recognition (NER) and noun chunks.\"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    key_phrases = set()\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        key_phrases.add(ent.text)\n",
        "\n",
        "    for chunk in doc.noun_chunks:\n",
        "        key_phrases.add(chunk.text)\n",
        "\n",
        "    return sorted(key_phrases, key=lambda x: len(x.split()), reverse=True)\n",
        "\n",
        "def determine_flashcard_count(text):\n",
        "    \"\"\"Determines the number of flashcards based on text length.\"\"\"\n",
        "    word_count = len(text.split())\n",
        "    if word_count < 100:\n",
        "        return 3\n",
        "    elif 100 <= word_count < 300:\n",
        "        return 5\n",
        "    elif 300 <= word_count < 600:\n",
        "        return 7\n",
        "    else:\n",
        "        return 10\n",
        "\n",
        "def compute_sentence_scores(sentences):\n",
        "    \"\"\"Computes sentence importance using TF-IDF weighting.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "    return sentence_scores\n",
        "\n",
        "def textrank_summary(text, top_n):\n",
        "    \"\"\"Ranks sentences using a hybrid of TextRank and TF-IDF for optimal summarization.\"\"\"\n",
        "    sentences = preprocess_text(text)\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(sentences)\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "    textrank_scores = nx.pagerank(graph)\n",
        "\n",
        "    tfidf_scores = compute_sentence_scores(sentences)\n",
        "\n",
        "    combined_scores = {i: textrank_scores[i] + tfidf_scores[i] for i in range(len(sentences))}\n",
        "    ranked_sentences = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    selected_sentences = []\n",
        "    seen_phrases = set()\n",
        "\n",
        "    for i, _ in ranked_sentences:\n",
        "        sentence = sentences[i]\n",
        "        key_phrases = set(sentence.lower().split()[:5])\n",
        "        if not any(key_phrases & seen for seen in seen_phrases):\n",
        "            selected_sentences.append(sentence)\n",
        "            seen_phrases.add(frozenset(key_phrases))\n",
        "        if len(selected_sentences) >= top_n:\n",
        "            break\n",
        "\n",
        "    return selected_sentences\n",
        "\n",
        "def generate_flashcards(text):\n",
        "    \"\"\"Generates concise, key-phrase-based flashcards dynamically.\"\"\"\n",
        "    num_flashcards = determine_flashcard_count(text)\n",
        "    key_sentences = textrank_summary(text, num_flashcards)\n",
        "    key_phrases = extract_key_phrases(text)\n",
        "\n",
        "    flashcards = {}\n",
        "    used_phrases = set()\n",
        "\n",
        "    for sentence in key_sentences:\n",
        "        relevant_phrase = None\n",
        "        for phrase in key_phrases:\n",
        "            if phrase.lower() in sentence.lower() and phrase not in used_phrases:\n",
        "                relevant_phrase = phrase\n",
        "                used_phrases.add(phrase)\n",
        "                break\n",
        "\n",
        "        if not relevant_phrase or len(relevant_phrase.split()) < 2:  # Ensure meaningful selections\n",
        "            relevant_phrase = sentence.split('.')[0]\n",
        "\n",
        "        flashcards[relevant_phrase] = sentence\n",
        "\n",
        "    return flashcards\n",
        "\n",
        "# Get user input\n",
        "input_text = input(\"Enter your text to generate flashcards:\\n\")\n",
        "\n",
        "# Generate flashcards\n",
        "flashcards = generate_flashcards(input_text)\n",
        "\n",
        "# Print flashcards\n",
        "print(\"\\nðŸ“š Generated Flashcards:\\n\")\n",
        "for key_phrase, answer in flashcards.items():\n",
        "    print(f\"{answer}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7hHP3qht2xh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
